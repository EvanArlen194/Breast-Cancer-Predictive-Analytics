# -*- coding: utf-8 -*-
"""breast_cancer_model_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18gMo9-QnFvNvrZ7INJrsEqiNKqT_34eG

# Proyek Klasifikasi Kanker Payudara

Proyek ini bertujuan untuk menganalisis dan mengklasifikasikan data kanker payudara menggunakan beberapa algoritma machine learning. Dataset yang digunakan berasal dari UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set dengan link : https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data . Tujuan utama dari proyek ini adalah untuk mengklasifikasikan tumor sebagai **Malignant (ganas)** atau **Benign (jinak)** berdasarkan beberapa fitur medis yang tersedia. Model yang digunakan meliputi beberapa algoritma, dan evaluasi dilakukan dengan akurasi serta laporan klasifikasi.

# Langkah 1: Setup Kaggle API

Pada tahap ini, kita perlu mengautentikasi dan mengonfigurasi Kaggle API untuk mengunduh dataset. Kita akan mengunggah file `kaggle.json`, yang berisi kredensial yang diperlukan untuk mengakses dataset dari Kaggle.
"""

from google.colab import files

uploaded = files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

"""# Langkah 2: Mengunduh Dataset dari Kaggle

Setelah setup Kaggle API selesai, langkah berikutnya adalah mengunduh dataset "Breast Cancer Wisconsin (Diagnostic)" dari Kaggle dan mengekstraknya. Dataset ini akan digunakan untuk analisis dan pemodelan.

"""

# Mendownload dataset menggunakan Kaggle API
!kaggle datasets download -d uciml/breast-cancer-wisconsin-data

# Mengekstrak file zip yang telah diunduh
!unzip breast-cancer-wisconsin-data.zip

"""# Langkah 3: Mengimpor Library yang Diperlukan

Pada langkah ini, kita mengimpor library yang diperlukan untuk analisis data, visualisasi, dan machine learning.
- **Pandas** digunakan untuk memanipulasi dan menganalisis data.
- **NumPy** digunakan untuk perhitungan numerik.
- **Seaborn dan Matplotlib** digunakan untuk visualisasi.
- **Scikit-learn** digunakan untuk pemodelan machine learning.
- **EllipticEnvelope** digunakan untuk mendeteksi dan menangani pencilan secara statistik.
- **SMOTE** digunakan untuk mengatasi masalah ketidakseimbangan kelas dalam dataset.

"""

!pip install imblearn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.covariance import EllipticEnvelope
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix
from imblearn.over_sampling import SMOTE
import joblib
import warnings
warnings.filterwarnings('ignore')
sns.set(style="whitegrid")

"""# Langkah 4: Memuat dan Mengeksplorasi Data

Setelah dataset diunduh, kita akan memuat data ke dalam dataframe Pandas dan mengeksplorasi struktur dasar dataset, termasuk dimensi, tipe data, serta melihat beberapa baris pertama. Kita juga akan menghapus kolom yang tidak relevan seperti 'Unnamed: 32' dan 'id'.
"""

# Memuat dataset
data_breast = pd.read_csv('data.csv')

# Cek dimensi dataset
print(f'Dimensi dataset: {data_breast.shape}')

# Tampilkan informasi dataset
data_breast.info()

# Tampilkan beberapa baris pertama
data_breast.head()

# Hapus kolom yang tidak diperlukan
data_breast = data_breast.drop(columns=['Unnamed: 32', 'id'], axis=1)

# Tampilkan informasi dataset setelah menghapus kolom yang tidak diperlukan
data_breast.info()
data_breast.describe()

"""Selanjutnya, pencilan akan dideteksi dan dihapus menggunakan **EllipticEnvelope**.
- **EllipticEnvelope** digunakan untuk mendeteksi pencilan berdasarkan asumsi distribusi Gaussian multivariat.
- Data dengan pencilan akan dihapus agar tidak mempengaruhi model yang akan dilatih.
"""

numerical_features = data_breast.select_dtypes(include=['float64', 'int64']).columns

# Menggunakan EllipticEnvelope untuk mendeteksi dan menangani pencilan
outlier_detector = EllipticEnvelope(contamination=0.01)

# Menghapus pencilan berdasarkan fitur numerik
for feature in numerical_features:
    feature_data = data_breast[[feature]].values
    mask = outlier_detector.fit_predict(feature_data)
    data_breast = data_breast[mask == 1]

print(f"Ukuran dataset setelah pencilan dihapus: {data_breast.shape}")

"""# Langkah 5: Visualisasi Data (Distribusi Kelas dan Box Plot)
Pada langkah ini, kita memvisualisasikan distribusi target kelas 'diagnosis' serta distribusi fitur numerik dengan **box plot**.

- **Countplot** digunakan untuk memvisualisasikan distribusi target antara kategori Malignant dan Benign.
- **Boxplot** digunakan untuk memeriksa distribusi fitur numerik setelah pencilan dihapus.
"""

# Visualisasi distribusi diagnosis
plt.figure(figsize=(10, 6))
sns.countplot(x='diagnosis', data=data_breast, palette='Set1')
plt.title('Distribusi Diagnosis (Malignant vs Benign)')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))

# Visualisasi boxplot fitur numerik
sns.boxplot(data=data_breast[numerical_features], orient="h", palette="coolwarm")
plt.title('Box Plot Fitur Numerik Setelah Menghapus Pencilan')
plt.tight_layout()
plt.show()

"""# Langkah 6: Encoding Kolom Target dan Analisis Korelasi
Pada tahap ini, kita mengonversi kolom 'diagnosis' menjadi nilai numerik agar bisa digunakan dalam model machine learning.
- **M (Malignant)** dikonversi menjadi **1**, dan **B (Benign)** dikonversi menjadi **0**.
- **Matriks korelasi** divisualisasikan untuk melihat hubungan antar fitur dalam dataset.
"""

# Mengonversi kolom 'diagnosis' menjadi numerik (M -> 1, B -> 0)
data_breast['diagnosis'] = data_breast['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)

# Matriks korelasi
plt.figure(figsize=(20, 10))
correlation_matrix = data_breast.corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Matriks Korelasi Fitur")
plt.show()

"""# Langkah 7: Memisahkan Variabel Fitur dan Target
Pada langkah ini, kita memisahkan variabel fitur (X) dan variabel target (y) dari dataset. Variabel fitur adalah semua kolom kecuali kolom 'diagnosis', yang merupakan label untuk klasifikasi.
"""

X = data_breast.drop(['diagnosis'], axis=1)
y = data_breast['diagnosis']

"""# Langkah 8: Memeriksa Unique Class Labels di Target
Kita perlu memeriksa kelas unik yang ada di variabel target (y) untuk memastikan bahwa dataset memiliki lebih dari satu kelas. Ini penting untuk memastikan bahwa model dapat dilatih dengan baik.
"""

unique_classes = np.unique(y)
print(f"Unique class labels in y: {unique_classes}")

"""# Langkah 9: Memeriksa Distribusi Kelas di y_train
Setelah membagi dataset, kita memeriksa distribusi kelas di data pelatihan (y_train) untuk melihat apakah kita memiliki proporsi yang seimbang antara kelas yang ada.
"""

# Membagi dataset menjadi 70:30
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print("Distribusi kelas di y_train:")
print(y_train.value_counts())

"""# Langkah 10: Menggunakan SMOTE untuk Oversampling
Sebelum menerapkan SMOTE (Synthetic Minority Over-sampling Technique), kita memeriksa apakah ada lebih dari satu kelas dalam y_train. Jika ada, kita menggunakan SMOTE untuk menyeimbangkan kelas dengan membuat contoh sintetis dari kelas minoritas. Jika SMOTE diterapkan, kita akan melihat distribusi kelas di y_train_balanced untuk memastikan bahwa kelas sudah seimbang setelah proses oversampling.
"""

# Memastikan ada lebih dari satu kelas
if len(np.unique(y_train)) > 1:
    smote = SMOTE(random_state=42)
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

    # Melihat Distribusi Kelas Setelah Oversampling
    print("Distribusi kelas setelah SMOTE:")
    print(pd.Series(y_train_balanced).value_counts())
else:
    print("Hanya ada satu kelas di y_train, tidak ada SMOTE yang diterapkan.")

"""# Langkah 11: Normalisasi Data

Pada langkah ini, data fitur dinormalisasi menggunakan **StandardScaler** agar setiap fitur berada dalam skala yang sama. Ini penting untuk model seperti SVM dan KNN yang sensitif terhadap skala data.
"""

# Normalisasi fitur menggunakan StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Distribusi kelas di y_train:")
print(y_train.value_counts())
print("\nDistribusi kelas di y_test:")
print(y_test.value_counts())

"""# Langkah 12: Melatih Berbagai Model dan Memilih Model Terbaik
Pada langkah ini, berbagai model machine learning dilatih menggunakan set pelatihan yang telah dinormalisasi. Model yang digunakan meliputi:
- **Support Vector Classifier**
- **K-Nearest Neighbors**
- **Decision Tree**

Setiap model dievaluasi berdasarkan akurasi pada set pengujian, dan model dengan akurasi tertinggi dipilih sebagai model terbaik.
"""

# Definisikan model yang akan digunakan
models = {
    'Support Vector Classifier': SVC(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree Classifier': DecisionTreeClassifier()
}

# Inisialisasi dictionary untuk menyimpan hasil
results = {}

# Melatih dan mengevaluasi tiap model
for model_name, model in models.items():
    # Melatih model
    model.fit(X_train_scaled, y_train)

    # Prediksi dengan model
    y_pred = model.predict(X_test_scaled)

    # Menghitung akurasi
    acc = accuracy_score(y_test, y_pred)*100
    results[model_name] = acc

    # Menampilkan hasil
    print(f"\n### {model_name} ###")
    print(f"Accuracy: {acc:.2f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

# Menampilkan hasil akurasi
for model_name, acc in results.items():
    print(f"{model_name}: Akurasi = {acc:.2f}")

"""# Langkah 13: Visualisasi Hasil Akurasi Model

Pada langkah ini, kita memvisualisasikan akurasi model yang dilatih menggunakan **bar plot**. Ini membantu kita membandingkan performa masing-masing model.
"""

plt.figure(figsize=(10, 6))
sns.barplot(x=list(results.keys()), y=list(results.values()), palette="viridis")
plt.title("Akurasi Model")
plt.ylabel("Akurasi")
plt.xlabel("Model")
plt.xticks(rotation=45)
plt.show()

"""# Langkah 14: Menyimpan Model Terbaik

Setelah model terbaik dipilih, model disimpan menggunakan **joblib** agar dapat digunakan kembali di masa depan tanpa perlu melatih ulang.
"""

import joblib

# Mencari model dengan akurasi tertinggi
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]

# Menyimpan model terbaik
joblib.dump(best_model, 'model_terbaik.pkl')
print("Model terbaik telah disimpan!")

"""# Langkah 15: Menguji Model Terbaik
Pada langkah ini, kita akan memuat model terbaik yang telah disimpan dan mengujinya kembali menggunakan data pengujian untuk memastikan bahwa model tersebut bekerja dengan baik setelah disimpan. Pengujian ini dilakukan dengan melakukan prediksi pada data uji yang telah disiapkan, kemudian menampilkan hasil akurasi dan classification report untuk evaluasi performa model.
"""

# Memuat model terbaik dari file
best_model_loaded = joblib.load('model_terbaik.pkl')

# Melakukan prediksi menggunakan data uji
y_pred_loaded = best_model_loaded.predict(X_test_scaled)

# Menghitung akurasi model yang di-load
acc_loaded = accuracy_score(y_test, y_pred_loaded)

# Menampilkan akurasi dan laporan klasifikasi
print(f"Accuracy of the loaded model: {acc_loaded:.4f}")
print("Classification Report of the loaded model:")
print(classification_report(y_test, y_pred_loaded))

"""# Langkah 16: Menampilkan Hasil Pengujian
Pada langkah terakhir ini, kita menampilkan hasil pengujian dari model yang telah diload dengan beberapa metrik penting seperti akurasi, precision, recall, dan f1-score untuk setiap kelas (Malignant dan Benign). Selain itu, kita juga menampilkan matriks kebingungan (confusion matrix) untuk melihat distribusi hasil prediksi dibandingkan dengan label yang sebenarnya.
"""

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_loaded)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for the Best Model')
plt.show()